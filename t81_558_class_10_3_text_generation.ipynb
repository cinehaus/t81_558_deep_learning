{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqzVtVWq4Z7Y"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_3_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pwTvlD_Z4Z7a"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "\n",
        "**Module 10: Time Series in Keras**\n",
        "\n",
        "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dg5XHOR64Z7a"
      },
      "source": [
        "# Module 10 Material\n",
        "\n",
        "- Part 10.1: Time Series Data Encoding for Deep Learning [[Video]](https://www.youtube.com/watch?v=dMUmHsktl04&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_1_timeseries.ipynb)\n",
        "- Part 10.2: Programming LSTM with Keras and TensorFlow [[Video]](https://www.youtube.com/watch?v=wY0dyFgNCgY&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_2_lstm.ipynb)\n",
        "- **Part 10.3: Text Generation with Keras and TensorFlow** [[Video]](https://www.youtube.com/watch?v=6ORnRAz3gnA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_3_text_generation.ipynb)\n",
        "- Part 10.4: Introduction to Transformers [[Video]](https://www.youtube.com/watch?v=Z7FIdKVQ7kc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_4_intro_transformers.ipynb)\n",
        "- Part 10.5: Transformers for Timeseries [[Video]](https://www.youtube.com/watch?v=SX67Mni0Or4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_5_keras_transformers.ipynb)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1X6uEpSF4Z7b"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzvMKJSx4Z7b",
        "outputId": "be9cf42e-7fa4-4f33-8fb7-700e8774a831"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MpZejB594Z7c"
      },
      "source": [
        "# Part 10.3: Text Generation with LSTM\n",
        "\n",
        "Recurrent neural networks are also known for their ability to generate text. As a result, the neural network's output can be free-form text. This section will demonstrate how to train an LSTM on a textual document, such as classic literature, and learn to output new text that appears to be of the same form as the training material. If you train your LSTM on [Shakespeare](https://en.wikipedia.org/wiki/William_Shakespeare), it will learn to crank out new prose similar to what Shakespeare had written.\n",
        "\n",
        "Don't get your hopes up. You will not teach your deep neural network to write the next [Pulitzer Prize for Fiction](https://en.wikipedia.org/wiki/Pulitzer_Prize_for_Fiction). The prose generated by your neural network will be nonsensical. However, the output text will usually be nearly grammatically correct and similar to the source training documents.\n",
        "\n",
        "A neural network generating nonsensical text based on literature may not seem helpful. However, this technology gets so much interest because it forms the foundation for many more advanced technologies. The LSTM will typically learn human grammar from the source document opens a wide range of possibilities. You can use similar technology to complete sentences when entering text. The ability to output free-form text has become the foundation of many other technologies. In the next part, we will use this technique to create a neural network that can write captions for images to describe what is going on in the picture.\n",
        "\n",
        "## Additional Information\n",
        "\n",
        "The following are some articles that I found helpful in putting this section together.\n",
        "\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "- [Keras LSTM Generation Example](https://keras.io/examples/lstm_text_generation/)\n",
        "\n",
        "## Character-Level Text Generation\n",
        "\n",
        "There are several different approaches to teaching a neural network to output free-form text. The most basic question is if you wish the neural network to learn at the word or character level. Learning at the character level is the more interesting of the two. The LSTM is learning to construct its own words without even being shown what a word is. We will begin with character-level text generation. In the next module, we will see how we can use nearly the same technique to operate at the word level. We will implement word-level automatic captioning in the next module.\n",
        "\n",
        "We import the needed Python packages and define the sequence length, named **maxlen**. Time-series neural networks always accept their input as a fixed-length array. Because you might not use all of the sequence elements, filling extra pieces with zeros is common. You will divide the text into sequences of this length, and the neural network will train to predict what comes after this sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IISeTbGd4Z7c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "36f2PsYG4Z7d"
      },
      "source": [
        "We will train the neural network on the classic children's book [Treasure Island](https://en.wikipedia.org/wiki/Treasure_Island). We begin by loading this text into a Python string and displaying the first 1,000 characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iiADrDO4Z7d",
        "outputId": "603909d1-f766-40aa-da20-3b4644aefa3b"
      },
      "outputs": [],
      "source": [
        "r = requests.get(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/text/treasure_island.txt\"\n",
        ")\n",
        "raw_text = r.text\n",
        "print(raw_text[0:1000])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MaGzMiHU4Z7e"
      },
      "source": [
        "We will extract all unique characters from the text and sort them. This technique allows us to assign a unique ID to each character. Because we sorted the characters, these IDs should remain the same. The IDs will change if we add new characters to the original text. We build two dictionaries. The first **char2idx** is used to convert a character into its ID. The second **idx2char** converts an ID back into its character.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHWO-8uw4Z7e",
        "outputId": "f8016251-d473-498f-a5dc-c64654d50228"
      },
      "outputs": [],
      "source": [
        "processed_text = raw_text.lower()\n",
        "processed_text = re.sub(r\"[^\\x00-\\x7f]\", r\"\", processed_text)\n",
        "\n",
        "print(\"corpus length:\", len(processed_text))\n",
        "\n",
        "chars = sorted(list(set(processed_text)))\n",
        "print(\"total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w-IMZoYE4Z7e"
      },
      "source": [
        "We are now ready to build the actual sequences. Like previous neural networks, there will be an $x$ and $y$. However, for the LSTM, $x$ and $y$ will be sequences. The $x$ input will specify the sequences where $y$ is the expected output. The following code generates all possible sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL9KEhtt4Z7e",
        "outputId": "063aec1a-5deb-479a-bf88-3fa768e8a41c"
      },
      "outputs": [],
      "source": [
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(processed_text) - maxlen, step):\n",
        "    sentences.append(processed_text[i : i + maxlen])\n",
        "    next_chars.append(processed_text[i + maxlen])\n",
        "print(\"nb sequences:\", len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-0kdl3y4Z7e",
        "outputId": "95f992d9-2a15-48fc-c737-887aa8b6b115"
      },
      "outputs": [],
      "source": [
        "sentences"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhf8H3io4Z7f"
      },
      "source": [
        "We can now convert the text into vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeQGz4Zy4Z7f",
        "outputId": "d3d71bc7-d51a-4d4b-9609-65840ec1536a"
      },
      "outputs": [],
      "source": [
        "print(\"Vectorization...\")\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4XYPfC6B4Z7f"
      },
      "source": [
        "Next, we create the neural network. This neural network's primary feature is the LSTM layer, which allows the sequences to be processed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X-LHYrQ4Z7f",
        "outputId": "79a1ec2b-ce29-4dae-8d6c-b4fc01413d14"
      },
      "outputs": [],
      "source": [
        "# build the model: a single LSTM\n",
        "print(\"Build model...\")\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation=\"softmax\"))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRiYJutp4Z7f",
        "outputId": "bde1ceeb-d23a-4af7-a26b-bbf01e3b3273"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ODs3o9dD4Z7f"
      },
      "source": [
        "The LSTM will produce new text character by character. We will need to sample the correct letter from the LSTM predictions each time. The **sample** function accepts the following two parameters:\n",
        "\n",
        "- **preds** - The output neurons.\n",
        "- **temperature** - 1.0 is the most conservative, 0.0 is the most confident (willing to make spelling and other errors).\n",
        "\n",
        "The sample function below essentially performs a softmax on the neural network predictions. This process causes each output neuron to become a probability of its particular letter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si827V4C4Z7f"
      },
      "outputs": [],
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gIKmvGMa4Z7g"
      },
      "source": [
        "Keras calls the following function at the end of each training Epoch. The code generates sample text generations that visually demonstrate the neural network better at text generation. As the neural network trains, the generations should look more realistic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rnz5cBIe4Z7g"
      },
      "outputs": [],
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print(\"******************************************************\")\n",
        "    print(\"----- Generating text after Epoch: %d\" % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(processed_text) - maxlen - 1)\n",
        "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print(\"----- temperature:\", temperature)\n",
        "\n",
        "        generated = \"\"\n",
        "        sentence = processed_text[start_index : start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, temperature)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yq4Qoj_j4Z7g"
      },
      "source": [
        "We are now ready to train. Depending on how fast your computer is, it can take up to an hour to train this network. If you have a GPU available, please make sure to use it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QonqIsYe4Z7g",
        "outputId": "df752421-fce3-4c82-e5af-ef346fa1bd91"
      },
      "outputs": [],
      "source": [
        "# Ignore useless W0819 warnings generated by TensorFlow 2.0.  Hopefully can remove this ignore in the future.\n",
        "# See https://github.com/tensorflow/tensorflow/issues/31308\n",
        "import logging, os\n",
        "\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# Fit the model\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y, batch_size=128, epochs=60, callbacks=[print_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu4PtpxK4Z7g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "t81_558_class_10_3_text_generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
