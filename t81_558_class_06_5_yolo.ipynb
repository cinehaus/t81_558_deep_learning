{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XKzF6dMaiLyP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_5_yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YDTXd8-Lmp8Q"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "\n",
        "**Module 6: Convolutional Neural Networks (CNN) for Computer Vision**\n",
        "\n",
        "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ncNrAEpzmp8S"
      },
      "source": [
        "# Module 6 Material\n",
        "\n",
        "- Part 6.1: Image Processing in Python [[Video]](https://www.youtube.com/watch?v=V-IUrfTJMm4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_1_python_images.ipynb)\n",
        "- Part 6.2: Using Convolutional Neural Networks [[Video]](https://www.youtube.com/watch?v=nU_T2PPigUQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_2_cnn.ipynb)\n",
        "- Part 6.3: Using Pretrained Neural Networks with Keras [[Video]](https://www.youtube.com/watch?v=TXqI9fp0imI&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_3_resnet.ipynb)\n",
        "- Part 6.4: Looking at Keras Generators and Image Augmentation [[Video]](https://www.youtube.com/watch?v=epfpxiXRL3U&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_4_keras_images.ipynb)\n",
        "- **Part 6.5: Recognizing Multiple Images with YOLOv5** [[Video]](https://www.youtube.com/watch?v=zwEmzElquHw&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_5_yolo.ipynb)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z0stsqSVoKD0"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
        "Running the following code will map your GDrive to `/content/drive`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU9UhAxTmp8S",
        "outputId": "afccb25f-6fdd-4b62-fc9b-fa32e3ee1aad"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QSKZqD1Mmp-C"
      },
      "source": [
        "# Part 6.5: Recognizing Multiple Images with YOLO5\n",
        "\n",
        "Programmers typically design convolutional neural networks to classify a single item centered in an image. However, as humans, we can recognize many items in our field of view in real-time. It is advantageous to recognize multiple items in a single image. One of the most advanced means of doing this is YOLOv5. You Only Look Once (YOLO) was introduced by Joseph Redmon, who supported YOLO up through V3. [[Cite:redmon2016you]](https://arxiv.org/abs/1506.02640) The fact that YOLO must only look once speaks to the efficiency of the algorithm. In this context, to \"look\" means to perform one scan over the image. It is also possible to run YOLO on live video streams.\n",
        "\n",
        "Joseph Redmon left computer vision to pursue other interests. The current version, YOLOv5 is supported by the startup company [Ultralytics](https://ultralytics.com/), who released the open-source library that we use in this class.[[Cite:zhu2021tph]](https://arxiv.org/abs/2108.11539)\n",
        "\n",
        "Researchers have trained YOLO on a variety of different computer image datasets. The version of YOLO weights used in this course is from the dataset Common Objects in Context (COCO). [[Cite: lin2014microsoft]](https://arxiv.org/abs/1405.0312) This dataset contains images labeled into 80 different classes. COCO is the source of the file coco.txt used in this module.\n",
        "\n",
        "## Using YOLO in Python\n",
        "\n",
        "To use YOLO in Python, we will use the open-source library provided by Ultralytics.\n",
        "\n",
        "- [YOLOv5 GitHub](https://github.com/ultralytics/yolov5)\n",
        "\n",
        "The code provided in this notebook works equally well when run either locally or from Google CoLab. It is easier to run YOLOv5 from CoLab, which is recommended for this course.\n",
        "\n",
        "We begin by obtaining an image to classify.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "WXEobzvGlFim",
        "outputId": "dbd1cd58-63d1-4abc-bf9c-4ce1499c5bdc"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import shutil\n",
        "from IPython.display import Image\n",
        "!mkdir /content/images/\n",
        "\n",
        "URL = \"https://github.com/jeffheaton/t81_558_deep_learning\"\n",
        "URL += \"/raw/master/photos/jeff_cook.jpg\"\n",
        "LOCAL_IMG_FILE = \"/content/images/jeff_cook.jpg\"\n",
        "\n",
        "with urllib.request.urlopen(URL) as response, \\\n",
        "  open(LOCAL_IMG_FILE, 'wb') as out_file:\n",
        "    shutil.copyfileobj(response, out_file)\n",
        "\n",
        "Image(filename=LOCAL_IMG_FILE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym5_juokofQl"
      },
      "source": [
        "## Installing YOLOv5\n",
        "\n",
        "YOLO is not available directly through either PIP or CONDA. Additionally, YOLO is not installed in Google CoLab by default. Therefore, whether you wish to use YOLO through CoLab or run it locally, you need to go through several steps to install it. This section describes the process of installing YOLO. The same steps apply to either CoLab or a local install. For CoLab, you must repeat these steps each time the system restarts your virtual environment. You must perform these steps only once for your virtual Python environment for a local install. If you are installing locally, install to the same virtual environment you created for this course. The following commands install YOLO directly from its GitHub repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuTjby5MzEre",
        "outputId": "300d24a5-7c7e-44bb-a1c5-2f8d4dbdaec4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "!git clone https://github.com/ultralytics/yolov5 --tag 6.2  # clone\n",
        "!mv /content/6.2 /content/yolov5\n",
        "%pip install -qr /content/yolov5/requirements.txt  # install\n",
        "sys.path.insert(0,'/content/yolov5')\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9PSttcoraUlb"
      },
      "source": [
        "Next, we will run YOLO from the command line and classify the previously downloaded kitchen picture. You can run this classification on any image you choose.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "R8zq_6akz64w",
        "outputId": "eaa075f8-ff1f-4320-db10-5f5f315e4d2e"
      },
      "outputs": [],
      "source": [
        "# Prepare directories for YOLO command line\n",
        "!rm -R /content/yolov5/runs/detect/*\n",
        "!mkdir /content/images\n",
        "!cp /content/street/jeff_cook.jpg /content/images\n",
        "\n",
        "# Run YOLO to classify\n",
        "!python /content/yolov5/detect.py --weights yolov5s.pt --img 1024 \\\n",
        "  --conf 0.25 --source /content/images/\n",
        "\n",
        "# Display the images\n",
        "from IPython.display import Image\n",
        "\n",
        "URL = '/content/yolov5/runs/detect/exp/jeff_cook.jpg'\n",
        "Image(filename=URL, width=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MADv9eSnrBed",
        "outputId": "4e0bd648-2f4a-479d-d33b-40273322661c"
      },
      "outputs": [],
      "source": [
        "!ls /content/yolov5/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qYOvD3M7ofQl"
      },
      "source": [
        "## Running YOLOv5\n",
        "\n",
        "In addition to the command line execution, we just saw. The following code adds the downloaded YOLOv5 to Python's environment, allowing **yolov5** to be imported like a regular Python library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "MY3gVyidmp-K",
        "outputId": "3be3086b-8151-4325-dd7c-cbfdd90cdb1e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Model\n",
        "yolo_model = torch.hub.load(\n",
        "    \"ultralytics/yolov5\", \"yolov5s\"\n",
        ")  # or yolov5n - yolov5x6, custom\n",
        "\n",
        "# Inference\n",
        "results = yolo_model(LOCAL_IMG_FILE)\n",
        "\n",
        "# Results\n",
        "df = results.pandas().xyxy[0]\n",
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DL173C9_oKEU"
      },
      "source": [
        "It is important to note that the **yolo** class instantiated here is a callable object, which can fill the role of both an object and a function. Acting as a function, _yolo_ returns a Pandas dataframe that contains the bounding boxes (xmin/xmax and ymin/ymax), confidence, and name/class of each item detected.\n",
        "\n",
        "Your program should use these values to perform whatever actions you wish due to the input image. The following code displays the images detected above the threshold.\n",
        "\n",
        "You can obtain the counts of images through the use of a Pandas groupby and pivot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "Tg-Y0vTt-220",
        "outputId": "54d217bd-66e0-40ef-cb75-562bde8faa3b"
      },
      "outputs": [],
      "source": [
        "df2 = df[[\"name\", \"class\"]].groupby(by=[\"name\"]).count().reset_index()\n",
        "df2.columns = [\"name\", \"count\"]\n",
        "df2[\"image\"] = 1\n",
        "df2.pivot(index=[\"image\"], columns=\"name\", values=\"count\").reset_index().fillna(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0eBtaFbimp-M"
      },
      "source": [
        "# Module 6 Assignment\n",
        "\n",
        "You can find the first assignment here: [assignment 6](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class6.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
