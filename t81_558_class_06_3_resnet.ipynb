{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LSIM-PITWYFa"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_3_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YDTXd8-Lmp8Q"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "\n",
    "**Module 6: Convolutional Neural Networks (CNN) for Computer Vision**\n",
    "\n",
    "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ncNrAEpzmp8S"
   },
   "source": [
    "# Module 6 Material\n",
    "\n",
    "- Part 6.1: Image Processing in Python [[Video]](https://www.youtube.com/watch?v=V-IUrfTJMm4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_1_python_images.ipynb)\n",
    "- Part 6.2: Using Convolutional Neural Networks [[Video]](https://www.youtube.com/watch?v=nU_T2PPigUQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_2_cnn.ipynb)\n",
    "- **Part 6.3: Using Pretrained Neural Networks with Keras** [[Video]](https://www.youtube.com/watch?v=TXqI9fp0imI&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_3_resnet.ipynb)\n",
    "- Part 6.4: Looking at Keras Generators and Image Augmentation [[Video]](https://www.youtube.com/watch?v=epfpxiXRL3U&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_4_keras_images.ipynb)\n",
    "- Part 6.5: Recognizing Multiple Images with YOLOv5 [[Video]](https://www.youtube.com/watch?v=zwEmzElquHw&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_5_yolo.ipynb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lux_6KOXMU94"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU9UhAxTmp8S",
    "outputId": "a7dc7035-f775-4c3e-cded-e9cce17207e9"
   },
   "outputs": [],
   "source": [
    "# Detect Colab if present\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q09yMGGcmp9N"
   },
   "source": [
    "# Part 6.3: Transfer Learning for Computer Vision\n",
    "\n",
    "Many advanced prebuilt neural networks are available for computer vision, and Keras provides direct access to many networks. Transfer learning is the technique where you use these prebuilt neural networks. Module 9 takes a deeper look at transfer learning.\n",
    "\n",
    "There are several different levels of transfer learning.\n",
    "\n",
    "- Use a prebuilt neural network in its entirety\n",
    "- Use a prebuilt neural network's structure\n",
    "- Use a prebuilt neural network's weights\n",
    "\n",
    "We will begin by using the MobileNet prebuilt neural network in its entirety. MobileNet will be loaded and allowed to classify simple images. We can already classify 1,000 images through this technique without ever having trained the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpN-M-BScz5K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vB4Im0gKfF0J"
   },
   "source": [
    "We begin by downloading weights for a MobileNet trained for the imagenet dataset, which will take some time to download the first time you train the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUDtlNDxfMBA",
    "outputId": "0c9597ac-9385-4fda-f7cb-de18f82de4a9"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "model = MobileNet(weights=\"imagenet\", include_top=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cwuvm2W_fRZy"
   },
   "source": [
    "The loaded network is a Keras neural network. However, this is a neural network that a third party engineered on advanced hardware. Merely looking at the structure of an advanced state-of-the-art neural network can be educational.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YF5ZFnrqfXA5",
    "outputId": "7747c94b-0dc2-4dc0-a1bc-60313059a6de"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zcdKUTOAfbw0"
   },
   "source": [
    "Several clues to neural network architecture become evident when examining the above structure.\n",
    "\n",
    "We will now use the MobileNet to classify several image URLs below. You can add additional URLs of your own to see how well the MobileNet can classify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDX_FwNDfvlX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image, ImageFile\n",
    "from matplotlib.pyplot import imshow\n",
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "from tensorflow.keras.applications.mobilenet import decode_predictions\n",
    "\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "ROOT = \"https://data.heatonresearch.com/data/t81-558/images/\"\n",
    "\n",
    "def make_square(img):\n",
    "    cols,rows = img.size\n",
    "    \n",
    "    if rows>cols:\n",
    "        pad = (rows-cols)/2\n",
    "        img = img.crop((pad,0,cols,cols))\n",
    "    else:\n",
    "        pad = (cols-rows)/2\n",
    "        img = img.crop((0,pad,rows,rows))\n",
    "    \n",
    "    return img\n",
    "        \n",
    "\n",
    "def classify_image(url):\n",
    "  x = []\n",
    "  ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "  response = requests.get(url)\n",
    "  img = Image.open(BytesIO(response.content))\n",
    "  img.load()\n",
    "  img = img.resize((IMAGE_WIDTH,IMAGE_HEIGHT),Image.ANTIALIAS)\n",
    "\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "  x = x[:,:,:,:3] # maybe an alpha channel\n",
    "  pred = model.predict(x)\n",
    "\n",
    "  display(img)\n",
    "  print(np.argmax(pred,axis=1))\n",
    "\n",
    "  lst = decode_predictions(pred, top=5)\n",
    "  for itm in lst[0]:\n",
    "      print(itm)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GP6UnmqcjAAo"
   },
   "source": [
    "We can now classify an example image. You can specify the URL of any image you wish to classify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "FwfnWNXSo7Vk",
    "outputId": "f286ab26-7c13-489f-ccf4-c9c0c60b19c1"
   },
   "outputs": [],
   "source": [
    "classify_image(ROOT + \"soccer_ball.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "70FicXOvo0l0",
    "outputId": "dd9fa1a5-5b7f-4997-c9a6-7bdedf28b6a7"
   },
   "outputs": [],
   "source": [
    "classify_image(ROOT + \"race_truck.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP7lQFZ6f3VM"
   },
   "source": [
    "Overall, the neural network is doing quite well.\n",
    "\n",
    "For many applications, MobileNet might be entirely acceptable as an image classifier. However, if you need to classify very specialized images, not in the 1,000 image types supported by imagenet, it is necessary to use transfer learning.\n",
    "\n",
    "## Using the Structure of ResNet\n",
    "\n",
    "We will train a neural network to count the number of paper clips in images. We will make use of the structure of the ResNet neural network. There are several significant changes that we will make to ResNet to apply to this task. First, ResNet is a classifier; we wish to perform a regression to count. Secondly, we want to change the image resolution that ResNet uses. We will not use the weights from ResNet; changing this resolution invalidates the current weights. Thus, it will be necessary to retrain the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8ixjIi5p8Uy",
    "outputId": "f5d16e4c-a74b-4d05-d572-337bb6dae27a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "URL = \"https://github.com/jeffheaton/data-mirror/\"\n",
    "DOWNLOAD_SOURCE = URL + \"releases/download/v1/paperclips.zip\"\n",
    "DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind(\"/\") + 1 :]\n",
    "\n",
    "if COLAB:\n",
    "    PATH = \"/content\"\n",
    "else:\n",
    "    # I used this locally on my machine, you may need different\n",
    "    PATH = \"/Users/jeff/temp\"\n",
    "\n",
    "EXTRACT_TARGET = os.path.join(PATH, \"clips\")\n",
    "SOURCE = os.path.join(EXTRACT_TARGET, \"paperclips\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sj-mDxHekmio"
   },
   "source": [
    "Next, we download the images. This part depends on the origin of your images. The following code downloads images from a URL, where a ZIP file contains the images. The code unzips the ZIP file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqJXCf76qbR-",
    "outputId": "c24c5210-4184-43df-9307-34b933a7ed8d"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "!wget -O {os.path.join(PATH,DOWNLOAD_NAME)} {DOWNLOAD_SOURCE}\n",
    "!mkdir -p {SOURCE}\n",
    "!mkdir -p {TARGET}\n",
    "!mkdir -p {EXTRACT_TARGET}\n",
    "!unzip -o -j -d {SOURCE} {os.path.join(PATH, DOWNLOAD_NAME)} >/dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PX4uUN5DkuYH"
   },
   "source": [
    "The labels are contained in a CSV file named **train.csv** for the regression. This file has just two labels, **id** and **clip_count**. The ID specifies the filename; for example, row id 1 corresponds to the file **clips-1.jpg**. The following code loads the labels for the training set and creates a new column, named **filename**, that contains the filename of each image, based on the **id** column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOHDapz2sW_V"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(SOURCE, \"train.csv\"))\n",
    "df_train[\"filename\"] = \"clips-\" + df_train.id.astype(str) + \".jpg\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5NrCLEp8gQD9"
   },
   "source": [
    "We want to use early stopping. To do this, we need a validation set. We will break the data into 80 percent test data and 20 validation. Do not confuse this validation data with the test set provided by Kaggle. This validation set is unique to your program and is for early stopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOVV842BshWp",
    "outputId": "30876803-ce16-4aa9-f8bc-68e8a8c2b862"
   },
   "outputs": [],
   "source": [
    "TRAIN_PCT = 0.9\n",
    "TRAIN_CUT = int(len(df_train) * TRAIN_PCT)\n",
    "\n",
    "df_train_cut = df_train[0:TRAIN_CUT]\n",
    "df_validate_cut = df_train[TRAIN_CUT:]\n",
    "\n",
    "print(f\"Training size: {len(df_train_cut)}\")\n",
    "print(f\"Validate size: {len(df_validate_cut)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aj_umT9VgRzI"
   },
   "source": [
    "Next, we create the generators that will provide the images to the neural network during training. We normalize the images so that the RGB colors between 0-255 become ratios between 0 and 1. We also use the **flow_from_dataframe** generator to connect the Pandas dataframe to the actual image files. We see here a straightforward implementation; you might also wish to use some of the image transformations provided by the data generator.\n",
    "\n",
    "The **HEIGHT** and **WIDTH** constants specify the dimensions to which the image will be scaled (or expanded). It is probably not a good idea to expand the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYiMObEJsoof",
    "outputId": "d2043497-d895-441e-9fd4-19374f01135b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    horizontal_flip=True,\n",
    "    # vertical_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "train_generator = training_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train_cut,\n",
    "    directory=SOURCE,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"clip_count\",\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    # Keeping the training batch size small\n",
    "    # USUALLY increases performance\n",
    "    batch_size=32,\n",
    "    class_mode=\"raw\",\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "val_generator = validation_datagen.flow_from_dataframe(\n",
    "    dataframe=df_validate_cut,\n",
    "    directory=SOURCE,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"clip_count\",\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    # Make the validation batch size as large as you\n",
    "    # have memory for\n",
    "    batch_size=256,\n",
    "    class_mode=\"raw\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8rVZJBMIgqJq"
   },
   "source": [
    "We will now use a ResNet neural network as a basis for our neural network. We will redefine both the input shape and output of the ResNet model, so we will not transfer the weights. Since we redefine the input, the weights are of minimal value. We begin by loading, from Keras, the ResNet50 network. We specify **include_top** as False because we will change the input resolution. We also specify **weights** as false because we must retrain the network after changing the top input layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MJpmLyhtahJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "\n",
    "base_model = ResNet50(\n",
    "    include_top=False, weights=None, input_tensor=input_tensor, input_shape=None\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7xF9EKVFhAGl"
   },
   "source": [
    "Now we must add a few layers to the end of the neural network so that it becomes a regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXO3lTFIs_U1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "model = Model(inputs=base_model.input, outputs=Dense(1)(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ErCUZTv1hJDx"
   },
   "source": [
    "We train like before; the only difference is that we do not define the entire neural network here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nonHOozTtpD3",
    "outputId": "2a54ad35-0394-4730-85dc-d6fcf7778a4a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "# Important, calculate a valid step size for the validation dataset\n",
    "STEP_SIZE_VALID = val_generator.n // val_generator.batch_size\n",
    "\n",
    "model.compile(\n",
    "    loss=\"mean_squared_error\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[RootMeanSquaredError(name=\"rmse\")],\n",
    ")\n",
    "monitor = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-3,\n",
    "    patience=50,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=250,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[monitor],\n",
    "    verbose=1,\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of t81_558_class_06_3_resnet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
