{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3zt5vYf8K1AF"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_04_2_multi_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-8F-M_WK1AI"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "\n",
        "**Module 4: Training for Tabular Data**\n",
        "\n",
        "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DYijxVF5K1AJ"
      },
      "source": [
        "# Module 4 Material\n",
        "\n",
        "- Part 4.1: Encoding a Feature Vector for Keras Deep Learning [[Video]](https://www.youtube.com/watch?v=Vxz-gfs9nMQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_04_1_feature_encode.ipynb)\n",
        "- **Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC** [[Video]](https://www.youtube.com/watch?v=-f3bg9dLMks&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_04_2_multi_class.ipynb)\n",
        "- Part 4.3: Keras Regression for Deep Neural Networks with RMSE [[Video]](https://www.youtube.com/watch?v=wNhBUC6X5-E&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_04_3_regression.ipynb)\n",
        "- Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Neural Network Training [[Video]](https://www.youtube.com/watch?v=VbDg8aBgpck&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_04_4_backprop.ipynb)\n",
        "- Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch [[Video]](https://www.youtube.com/watch?v=wmQX1t2PHJc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_04_5_rmse_logloss.ipynb)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "By-ggcQxK1AJ"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19pvQwY3K1AK",
        "outputId": "335d3fbc-71d4-425a-e168-4c2d1efb117f"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF1dep09K1AL"
      },
      "source": [
        "# Part 4.2: Multiclass Classification with ROC and AUC\n",
        "\n",
        "The output of modern neural networks can be of many different forms. However, classically, neural network output has typically been one of the following:\n",
        "\n",
        "- **Binary Classification** - Classification between two possibilities (positive and negative). Common in medical testing, does the person has the disease (positive) or not (negative).\n",
        "- **Classification** - Classification between more than 2. The iris dataset (3-way classification).\n",
        "- **Regression** - Numeric prediction. How many MPG does a car get? (covered in next video)\n",
        "\n",
        "We will look at some visualizations for all three in this section.\n",
        "\n",
        "It is important to evaluate the false positives and negatives in the results produced by a neural network. We will now look at assessing error for both classification and regression neural networks.\n",
        "\n",
        "## Binary Classification and ROC Charts\n",
        "\n",
        "Binary classification occurs when a neural network must choose between two options: true/false, yes/no, correct/incorrect, or buy/sell. To see how to use binary classification, we will consider a classification system for a credit card company. This system will either \"issue a credit card\" or \"decline a credit card.\" This classification system must decide how to respond to a new potential customer.\n",
        "\n",
        "When you have only two classes that you can consider, the objective function's score is the number of false-positive predictions versus the number of false negatives. False negatives and false positives are both types of errors, and it is essential to understand the difference. For the previous example, issuing a credit card would be positive. A false positive occurs when a model decides to issue a credit card to someone who will not make payments as agreed. A false negative happens when a model denies a credit card to someone who would have made payments as agreed.\n",
        "\n",
        "Because only two options exist, we can choose the mistake that is the more serious type of error, a false positive or a false negative. For most banks issuing credit cards, a false positive is worse than a false negative. Declining a potentially good credit card holder is better than accepting a credit card holder who would cause the bank to undertake expensive collection activities.\n",
        "\n",
        "Consider the following program that uses the [wcbreast_wdbc dataset](https://data.heatonresearch.com/data/t81-558/wcbreast_wdbc.csv) to classify if a breast tumor is cancerous (malignant) or not (benign).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "VmjrLkDwK1AM",
        "outputId": "650cc2a4-d295-459d-aa40-d9e2d2950c88"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/wcbreast_wdbc.csv\",\n",
        "    na_values=[\"NA\", \"?\"],\n",
        ")\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 5)\n",
        "pd.set_option(\"display.max_rows\", 5)\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic_JG5KpK1AN"
      },
      "source": [
        "ROC curves can be a bit confusing. However, they are prevalent in analytics. It is essential to know how to read them. Even their name is confusing. Do not worry about their name; the receiver operating characteristic curve (ROC) comes from electrical engineering (EE).\n",
        "\n",
        "Binary classification is common in medical testing. Often you want to diagnose if someone has a disease. This diagnosis can lead to two types of errors, known as false positives and false negatives:\n",
        "\n",
        "- **False Positive** - Your test (neural network) indicated that the patient had the disease; however, the patient did not.\n",
        "- **False Negative** - Your test (neural network) indicated that the patient did not have the disease; however, the patient did have the disease.\n",
        "- **True Positive** - Your test (neural network) correctly identified that the patient had the disease.\n",
        "- **True Negative** - Your test (neural network) correctly identified that the patient did not have the disease.\n",
        "\n",
        "Figure 4.ETYP shows you these types of errors.\n",
        "\n",
        "**Figure 4.ETYP: Type of Error**\n",
        "![Type of Error](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_4_errors.png 'Type of Error')\n",
        "\n",
        "Neural networks classify in terms of the probability of it being positive. However, at what possibility do you give a positive result? Is the cutoff 50%? 90%? Where you set, this cutoff is called the threshold. Anything above the cutoff is positive; anything below is negative. Setting this cutoff allows the model to be more sensitive or specific:\n",
        "\n",
        "More info on Sensitivity vs. Specificity: [Khan Academy](https://www.youtube.com/watch?v=Z5TtopYX1Gc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "QfjMVDf2K1AO",
        "outputId": "9fefbe74-69f2-4c55-bad1-459a3c581c28",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import math\n",
        "\n",
        "mu1 = -2\n",
        "mu2 = 2\n",
        "variance = 1\n",
        "sigma = math.sqrt(variance)\n",
        "x1 = np.linspace(mu1 - 5*sigma, mu1 + 4*sigma, 100)\n",
        "x2 = np.linspace(mu2 - 5*sigma, mu2 + 4*sigma, 100)\n",
        "plt.plot(x1, stats.norm.pdf(x1, mu1, sigma)/1,color=\"green\", \n",
        "         linestyle='dashed')\n",
        "plt.plot(x2, stats.norm.pdf(x2, mu2, sigma)/1,color=\"red\")\n",
        "plt.axvline(x=-2,color=\"black\")\n",
        "plt.axvline(x=0,color=\"black\")\n",
        "plt.axvline(x=+2,color=\"black\")\n",
        "plt.text(-2.7,0.55,\"Sensitive\")\n",
        "plt.text(-0.7,0.55,\"Balanced\")\n",
        "plt.text(1.7,0.55,\"Specific\")\n",
        "plt.ylim([0,0.53])\n",
        "plt.xlim([-5,5])\n",
        "plt.legend(['Negative','Positive'])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ew7B8lNK1AO"
      },
      "source": [
        "We will now train a neural network for the Wisconsin breast cancer dataset. We begin by preprocessing the data. Because we have all numeric data, we compute a z-score for each column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m55_Ygs9K1AP"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "x_columns = df.columns.drop(\"diagnosis\").drop(\"id\")\n",
        "for col in x_columns:\n",
        "    df[col] = zscore(df[col])\n",
        "\n",
        "# Convert to numpy - Regression\n",
        "x = df[x_columns].values\n",
        "y = df[\"diagnosis\"].map({\"M\": 1, \"B\": 0}).values  # Binary classification,\n",
        "# M is 1 and B is 0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "as4rUCd1K1AP"
      },
      "source": [
        "We can now define two functions. The first function plots a confusion matrix. The second function plots a ROC chart.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25ys9moDK1AP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Plot a confusion matrix.\n",
        "# cm is the confusion matrix, names are the names of the classes.\n",
        "def plot_confusion_matrix(cm, names, title='Confusion matrix', \n",
        "                            cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(names))\n",
        "    plt.xticks(tick_marks, names, rotation=45)\n",
        "    plt.yticks(tick_marks, names)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "\n",
        "# Plot an ROC. pred - the predictions, y - the expected output.\n",
        "def plot_roc(pred,y):\n",
        "    fpr, tpr, _ = roc_curve(y, pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3zotNGspK1AQ"
      },
      "source": [
        "### ROC Chart Example\n",
        "\n",
        "The following code demonstrates how to implement a ROC chart in Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tchQMWrIK1AQ",
        "outputId": "e4be0d2b-d974-4917-bc0f-908917773c99"
      },
      "outputs": [],
      "source": [
        "# Classification neural network\n",
        "import numpy as np\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train/test\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    Dense(\n",
        "        100, input_dim=x.shape[1], activation=\"relu\", kernel_initializer=\"random_normal\"\n",
        "    )\n",
        ")\n",
        "model.add(Dense(50, activation=\"relu\", kernel_initializer=\"random_normal\"))\n",
        "model.add(Dense(25, activation=\"relu\", kernel_initializer=\"random_normal\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\", kernel_initializer=\"random_normal\"))\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "monitor = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=1e-3,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[monitor],\n",
        "    verbose=2,\n",
        "    epochs=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "_95P1kTBK1AQ",
        "outputId": "35b353e5-0eaf-412f-83a3-1970e8e07bb1"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(x_test)\n",
        "plot_roc(pred, y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yElFCdNQK1AR"
      },
      "source": [
        "### Multiclass Classification Error Metrics\n",
        "\n",
        "If you want to predict more than one outcome, you will need more than one output neuron. Because a single neuron can predict two results, a neural network with two output neurons is somewhat rare. If there are three or more outcomes, there will be three or more output neurons. The following sections will examine several metrics for evaluating classification error. We will assess the following classification neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD57aaWKK1AR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Read the data set\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
        "    na_values=[\"NA\", \"?\"],\n",
        ")\n",
        "\n",
        "# Generate dummies for job\n",
        "df = pd.concat([df, pd.get_dummies(df[\"job\"], prefix=\"job\")], axis=1)\n",
        "df.drop(\"job\", axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for area\n",
        "df = pd.concat([df, pd.get_dummies(df[\"area\"], prefix=\"area\")], axis=1)\n",
        "df.drop(\"area\", axis=1, inplace=True)\n",
        "\n",
        "# Missing values for income\n",
        "med = df[\"income\"].median()\n",
        "df[\"income\"] = df[\"income\"].fillna(med)\n",
        "\n",
        "# Standardize ranges\n",
        "df[\"income\"] = zscore(df[\"income\"])\n",
        "df[\"aspect\"] = zscore(df[\"aspect\"])\n",
        "df[\"save_rate\"] = zscore(df[\"save_rate\"])\n",
        "df[\"age\"] = zscore(df[\"age\"])\n",
        "df[\"subscriptions\"] = zscore(df[\"subscriptions\"])\n",
        "\n",
        "# Convert to numpy - Classification\n",
        "x_columns = df.columns.drop(\"product\").drop(\"id\")\n",
        "x = df[x_columns].values\n",
        "dummies = pd.get_dummies(df[\"product\"])  # Classification\n",
        "products = dummies.columns\n",
        "y = dummies.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aggbZoEUK1AR",
        "outputId": "6893844e-1126-4e7d-b6cb-34b2ab04ab4f"
      },
      "outputs": [],
      "source": [
        "# Classification neural network\n",
        "import numpy as np\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train/test\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    Dense(\n",
        "        100, input_dim=x.shape[1], activation=\"relu\", kernel_initializer=\"random_normal\"\n",
        "    )\n",
        ")\n",
        "model.add(Dense(50, activation=\"relu\", kernel_initializer=\"random_normal\"))\n",
        "model.add(Dense(25, activation=\"relu\", kernel_initializer=\"random_normal\"))\n",
        "model.add(Dense(y.shape[1], activation=\"softmax\", kernel_initializer=\"random_normal\"))\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "monitor = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=1e-3,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[monitor],\n",
        "    verbose=2,\n",
        "    epochs=1000,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vr9U9rgvK1AR"
      },
      "source": [
        "### Calculate Classification Accuracy\n",
        "\n",
        "Accuracy is the number of rows where the neural network correctly predicted the target class. Accuracy is only used for classification, not regression.\n",
        "\n",
        "$$ accuracy = \\frac{c}{N} $$\n",
        "\n",
        "Where $c$ is the number correct and $N$ is the size of the evaluated set (training or validation). Higher accuracy numbers are desired.\n",
        "\n",
        "As we just saw, by default, Keras will return the percent probability for each class. We can change these prediction probabilities into the actual iris predicted with **argmax**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldTptygpK1AR"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred, axis=1)\n",
        "# raw probabilities to chosen class (highest probability)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6_a5WcpJK1AS"
      },
      "source": [
        "Now that we have the actual iris flower predicted, we can calculate the percent accuracy (how many were correctly classified).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7ZN4mO5K1AS",
        "outputId": "8a2bf888-d1e2-4317-bc55-855b78b5db99"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_compare = np.argmax(y_test, axis=1)\n",
        "score = metrics.accuracy_score(y_compare, pred)\n",
        "print(\"Accuracy score: {}\".format(score))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1zlG17MKK1AS"
      },
      "source": [
        "### Calculate Classification Log Loss\n",
        "\n",
        "Accuracy is like a final exam with no partial credit. However, neural networks can predict a probability of each of the target classes. Neural networks will give high probabilities to predictions that are more likely. Log loss is an error metric that penalizes confidence in wrong answers. Lower log loss values are desired.\n",
        "\n",
        "The following code shows the output of predict_proba:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ucCNS9XAK1AS",
        "outputId": "76375262-9554-4105-9d2c-3b088732e2df"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# Don't display numpy in scientific notation\n",
        "np.set_printoptions(precision=4)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Generate predictions\n",
        "pred = model.predict(x_test)\n",
        "\n",
        "print(\"Numpy array of predictions\")\n",
        "display(pred[0:5])\n",
        "\n",
        "print(\"As percent probability\")\n",
        "print(pred[0] * 100)\n",
        "\n",
        "score = metrics.log_loss(y_test, pred)\n",
        "print(\"Log loss score: {}\".format(score))\n",
        "\n",
        "# raw probabilities to chosen class (highest probability)\n",
        "pred = np.argmax(pred, axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iU3NLdorK1AS"
      },
      "source": [
        "[Log loss](https://www.kaggle.com/wiki/LogarithmicLoss) is calculated as follows:\n",
        "\n",
        "$$ \\mbox{log loss} = -\\frac{1}{N}\\sum\\_{i=1}^N {( {y}\\_i\\log(\\hat{y}\\_i) + (1 - {y}\\_i)\\log(1 - \\hat{y}\\_i))} $$\n",
        "\n",
        "You should use this equation only as an objective function for classifications that have two outcomes. The variable y-hat is the neural network’s prediction, and the variable y is the known correct answer. In this case, y will always be 0 or 1. The training data have no probabilities. The neural network classifies it either into one class (1) or the other (0).\n",
        "\n",
        "The variable N represents the number of elements in the training set the number of questions in the test. We divide by N because this process is customary for an average. We also begin the equation with a negative because the log function is always negative over the domain 0 to 1. This negation allows a positive score for the training to minimize.\n",
        "\n",
        "You will notice two terms are separated by the addition (+). Each contains a log function. Because y will be either 0 or 1, then one of these two terms will cancel out to 0. If y is 0, then the first term will reduce to 0. If y is 1, then the second term will be 0.\n",
        "\n",
        "If your prediction for the first class of a two-class prediction is y-hat, then your prediction for the second class is 1 minus y-hat. Essentially, if your prediction for class A is 70% (0.7), then your prediction for class B is 30% (0.3). Your score will increase by the log of your prediction for the correct class. If the neural network had predicted 1.0 for class A, and the correct answer was A, your score would increase by log (1), which is 0. For log loss, we seek a low score, so a correct answer results in 0. Some of these log values for a neural network's probability estimate for the correct class:\n",
        "\n",
        "- -log(1.0) = 0\n",
        "- -log(0.95) = 0.02\n",
        "- -log(0.9) = 0.05\n",
        "- -log(0.8) = 0.1\n",
        "- -log(0.5) = 0.3\n",
        "- -log(0.1) = 1\n",
        "- -log(0.01) = 2\n",
        "- -log(1.0e-12) = 12\n",
        "- -log(0.0) = negative infinity\n",
        "\n",
        "As you can see, giving a low confidence to the correct answer affects the score the most. Because log (0) is negative infinity, we typically impose a minimum value. Of course, the above log values are for a single training set element. We will average the log values for the entire training set.\n",
        "\n",
        "The log function is useful to penalizing wrong answers. The following code demonstrates the utility of the log function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "g5Zv2tgNK1AT",
        "outputId": "f0861083-7809-406f-9970-4c83505ca14a"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib.pyplot import figure, show\n",
        "from numpy import arange, sin, pi\n",
        "\n",
        "#t = arange(1e-5, 5.0, 0.00001)\n",
        "#t = arange(1.0, 5.0, 0.00001) # computer scientists\n",
        "t = arange(0.0, 1.0, 0.00001)  # data     scientists\n",
        "\n",
        "fig = figure(1,figsize=(12, 10))\n",
        "\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax1.plot(t, np.log(t))\n",
        "ax1.grid(True)\n",
        "ax1.set_ylim((-8, 1.5))\n",
        "ax1.set_xlim((-0.1, 2))\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_title('log(x)')\n",
        "\n",
        "show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaRtddpcK1AT"
      },
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "A confusion matrix shows which predicted classes are often confused for the other classes. The vertical axis (y) represents the true labels and the horizontal axis (x) represents the predicted labels. When the true label and predicted label are the same, the highest values occur down the diagonal extending from the upper left to the lower right. The other values, outside the diagonal, represent incorrect predictions. For example, in the confusion matrix below, the value in row 2, column 1 shows how often the predicted value A occurred when it should have been B.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "1pvqhwRcK1AT",
        "outputId": "89d39b8b-af2d-4861-81da-905acb8c5528"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_compare, pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Normalize the confusion matrix by row (i.e by the number of samples\n",
        "# in each class)\n",
        "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "print(\"Normalized confusion matrix\")\n",
        "print(cm_normalized)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm_normalized, products, title=\"Normalized confusion matrix\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0KvHKadK1AT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "t81_558_class_04_2_multi_class.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
